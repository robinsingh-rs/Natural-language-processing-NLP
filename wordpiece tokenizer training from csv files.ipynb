{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "import inspect\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all csv file\n",
    "additional_datasets_path = \".csv\"\n",
    "csv_fileslist=glob.glob(additional_datasets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(csv_fileslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Suppose your CSV files are stored in a list `csv_files`\n",
    "data = []\n",
    "for file in tqdm(csv_fileslist):\n",
    "    df = pd.read_csv(file)\n",
    "    # Suppose the text data is in the column 'text'\n",
    "    data.extend(df['input'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[x for x in tqdm(data) if isinstance(x,str)]\n",
    "data=[x.lower() for x in tqdm(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=10000, #  tokenizer size 10K\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(data, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokenizer for future use\n",
    "tokenizer.save(\"tokenizer_10k_.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two ways\n",
    "tokenizer_from_json=Tokenizer.from_file(\"tokenizer_10k_.json\")\n",
    "temp1=WordpieceTokenizer(\"tokenizer_10k_.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_text=\"today is my annivrdry\"\n",
    "output_hf= tokenizer_from_json.encode(ip_text)\n",
    "output_json=temp1.tokenize(ip_text)\n",
    "print(output_hf.tokens)\n",
    "print(output_json)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
